{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`launch.json` 文件夹的路径配置\n",
    "```json\n",
    "${workspaceRoot} // 当前打开的文件夹的绝对路径\n",
    "\n",
    "${workspaceRootFolderName} // 当前打开的文件夹的名字\n",
    "\n",
    "${file} // 当前打开正在编辑的文件名，包括绝对路径，文件名，文件后缀名\n",
    "\n",
    "${relativeFile} // 从当前打开的文件夹到当前打开的文件的路径\n",
    "\n",
    "${fileBasename} // 当前打开的文件名+后缀名，不包括路径\n",
    "\n",
    "${fileBasenameNoExtension} // 当前打开的文件的文件名，不包括路径和后缀名\n",
    "\n",
    "${fileDirname} // 当前打开的文件所在的绝对路径，不包括文件名\n",
    "\n",
    "${fileExtname} // 当前打开的文件的后缀名\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `data.py`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) - 1\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)\n",
    "```\n",
    "将文本中出现的所有 `word` 和它第一次出现时的次序 `idx` 存储\n",
    "\n",
    "```python\n",
    "\n",
    "\n",
    "class Corpus():\n",
    "\n",
    "    def __init__(self, path):\n",
    "        self.dictionary = Dictionary()\n",
    "        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n",
    "        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n",
    "        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n",
    "\n",
    "    def tokenize(self, path):\n",
    "        assert os.path.exists(path)\n",
    "        with open(path, 'r', encoding='utf8') as f:\n",
    "            all_words = []\n",
    "            for line in f.readlines():\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    idx = self.dictionary.add_word(word)\n",
    "                    all_words.append(idx)\n",
    "        return torch.Tensor(all_words)\n",
    "```\n",
    "\n",
    "将整个 txt 文件的 `word` 映射为 `idx` 一维向量\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `model.py`\n",
    "## `RNNModel`\n",
    "```python\n",
    "- 原始输入: idx 组成的时序张量 `[idx1, idx2, ..., idxt]`\n",
    "- `self.encoder`: `self.encoder = nn.Embedding(ntoken, ninp)`: 将 `idx` embedding 到维度为 `ninp` 的向量空间\n",
    "- `self.rnn`: \n",
    "    - LSTM: 使用`nn`里的`LSTM`:  `self.rnn = getattr(nn, rnn_type)(ninp, nhid, nlayers, dropout=dropout)`\n",
    "    - GRU: 使用 `nn` 里的 `GRU`: `self.rnn = getattr(nn, rnn_type)(ninp, nhid, nlayers, dropout=dropout)`\n",
    "    - RNN_TANH: 手动设置 `nonlinearity` 为 `tanh`, 调用 `nn` 里的 `RNN`: `self.rnn = nn.RNN(ninp, nhid, nlayers, dropout=dropout, nonlinearity=nonlinearity)`\n",
    "    - RNN_RELU: 手动设置 `nonlinearity` 为 `relu`, 调用 `nn` 里的 `RNN`: `self.rnn = nn.RNN(ninp, nhid, nlayers, dropout=dropout, nonlinearity=nonlinearity)`\n",
    "\n",
    "- `self.decoder`: `self.decoder = nn.Linear(ninp, ntoken)`, 使用一个全连接层将输出映射为 `ntoken` 维\n",
    "class RNNModel(nn.Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5, tie_weights=False):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.ntoken = ntoken\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        if rnn_type in ['LSTM', 'GRU']:\n",
    "            self.rnn = getattr(nn, rnn_type)(ninp, nhid, nlayers, dropout=dropout)\n",
    "        else:\n",
    "            try:\n",
    "                nonlinearity = {'RNN_TANH': 'tanh', 'RNN_RELU': 'relu'}[rnn_type]\n",
    "            except KeyError:\n",
    "                raise ValueError( \"\"\"An invalid option for `--model` was supplied,\n",
    "                                 options are ['LSTM', 'GRU', 'RNN_TANH' or 'RNN_RELU']\"\"\")\n",
    "            self.rnn = nn.RNN(ninp, nhid, nlayers, nonlinearity=nonlinearity, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "\n",
    "        # Optionally tie weights as in:\n",
    "        # \"Using the Output Embedding to Improve Language Models\" (Press & Wolf 2016)\n",
    "        # https://arxiv.org/abs/1608.05859\n",
    "        # and\n",
    "        # \"Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling\" (Inan et al. 2016)\n",
    "        # https://arxiv.org/abs/1611.01462\n",
    "        if tie_weights:\n",
    "            if nhid != ninp:\n",
    "                raise ValueError('When using the tied flag, nhid must be equal to emsize')\n",
    "            # TODO: 这两个权重的维度应该是转置的关系吧\n",
    "            self.decoder.weight = self.encoder.weight\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        # TODO: 为什么不初始化 self.encoder.bias 是 Embedding 没有这个参数吗\n",
    "        nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n",
    "        nn.init.zeros_(self.decoder.bias)\n",
    "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        emb = self.drop(self.encoder(input))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output)\n",
    "        decoded = decoded.view(-1, self.ntoken)\n",
    "        return F.log_softmax(decoded, dim=1), hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters())\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
    "                    weight.new_zeros(self.nlayers, bsz, self.nhid))\n",
    "        else:\n",
    "            return weight.new_zeros(self.nlayers, bsz, self.nhid)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.arange(12).reshape(1, 3, 4)\n",
    "len(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `transformer`\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
